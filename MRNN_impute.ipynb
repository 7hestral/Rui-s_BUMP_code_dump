{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from MRNN.main_mrnn import *\n",
    "# combined_data_lst = ['combined_oura.csv', 'user_581_survey_data.csv']\n",
    "result_lst = []\n",
    "for f in combined_data_lst:\n",
    "# for f in os.listdir(os.path.join('.', 'dataset')):\n",
    "    # if 'combine' in f:\n",
    "    #     continue\n",
    "    file_name = './dataset/' + f\n",
    "    #file_name = './MRNN/data/google.csv'\n",
    "    test_df = pd.read_csv(file_name)\n",
    "    seq_len = 10\n",
    "    # seq_len = 50\n",
    "    h_dim = 10\n",
    "    batch_size = 8\n",
    "    iteration = 20\n",
    "    learning_rate = 0.01\n",
    "    metric_name = 'rmse'\n",
    "    #0.1, 0.3, 0.5, \n",
    "    for missing_rate in [0.5]:\n",
    "        result_lst.append(main({\n",
    "            'file_name': file_name,\n",
    "            'seq_len': seq_len,\n",
    "            'missing_rate': missing_rate,\n",
    "            'h_dim': h_dim,\n",
    "            'batch_size': batch_size,\n",
    "            'iteration': iteration,\n",
    "            'learning_rate': learning_rate,\n",
    "            'metric_name': metric_name\n",
    "        }))\n",
    "#   Args:\n",
    "#     - file_name: dataset file name\n",
    "#     - seq_len: sequence length of time-series data\n",
    "#     - missing_rate: the rate of introduced missingness\n",
    "#     - h_dim: hidden state dimensions\n",
    "#     - batch_size: the number of samples in mini batch\n",
    "#     - iteration: the number of iteration\n",
    "#     - learning_rate: learning rate of model training\n",
    "#     - metric_name: imputation performance metric (mse, mae, rmse)\n",
    "    \n",
    "# !cd ./MRNN\n",
    "# !python3 ./MRNN/main_mrnn.py --file_name ./MRNN/data/google.csv --seq_len 7 --missing_rate 0.2 --h_dim 10 --batch_size 128 --iteration 2000 --learning_rate 0.01 --metric_name rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "class GridSearcher:\n",
    "    def __init__(self, param_grid):\n",
    "        self.param_grid = [param_grid]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"Iterate over the points in the grid.\n",
    "        Returns\n",
    "        -------\n",
    "        params : iterator over dict of str to any\n",
    "            Yields dictionaries mapping each estimator parameter to one of its\n",
    "            allowed values.\n",
    "        \"\"\"\n",
    "        for p in self.param_grid:\n",
    "            # Always sort the keys of a dictionary, for reproducibility\n",
    "            items = sorted(p.items())\n",
    "            if not items:\n",
    "                yield {}\n",
    "            else:\n",
    "                keys, values = zip(*items)\n",
    "                for v in product(*values):\n",
    "                    params = dict(zip(keys, v))\n",
    "                    yield params\n",
    "param_grid = {\n",
    "    'seq_len': [5, 10, 20],\n",
    "    'h_dim': [5, 10, 20],\n",
    "    'batch_size': [8, 16, 32],\n",
    "    'iteration': [50, 200, 500],\n",
    "    'learning_rate': [0.01, 0.001]\n",
    "}\n",
    "my_searcher = GridSearcher(param_grid)\n",
    "for g in my_searcher:\n",
    "    print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from MRNN.main_mrnn import *\n",
    "# combined_data_lst = ['combined_oura.csv', 'user_581_survey_data.csv']\n",
    "param_grid_result_lst = []\n",
    "for f in combined_data_lst:\n",
    "# for f in os.listdir(os.path.join('.', 'dataset')):\n",
    "    # if 'combine' in f:\n",
    "    #     continue\n",
    "    file_name = './dataset/' + f\n",
    "    #file_name = './MRNN/data/google.csv'\n",
    "    test_df = pd.read_csv(file_name)\n",
    "    # seq_len = 10\n",
    "    # # seq_len = 50\n",
    "    # h_dim = 10\n",
    "    # batch_size = 8\n",
    "    # iteration = 20\n",
    "    # learning_rate = 0.01\n",
    "    metric_name = 'rmse'\n",
    "    #0.1, 0.3, 0.5, \n",
    "    for parameters in my_searcher:\n",
    "        for missing_rate in [0.1, 0.3, 0.5]:\n",
    "            try:\n",
    "                performance = main({\n",
    "                    'file_name': file_name,\n",
    "                    'seq_len': parameters['seq_len'],\n",
    "                    'missing_rate': missing_rate,\n",
    "                    'h_dim': parameters['h_dim'],\n",
    "                    'batch_size': parameters['batch_size'],\n",
    "                    'iteration': parameters['iteration'],\n",
    "                    'learning_rate': parameters['learning_rate'],\n",
    "                    'metric_name': metric_name\n",
    "                })['performance']\n",
    "            except:\n",
    "                performance = np.nan\n",
    "            param_grid_result_lst.append({\n",
    "                'file_name': f,\n",
    "                'seq_len': parameters['seq_len'],\n",
    "                'missing_rate': missing_rate,\n",
    "                'h_dim': parameters['h_dim'],\n",
    "                'batch_size': parameters['batch_size'],\n",
    "                'iteration': parameters['iteration'],\n",
    "                'learning_rate': parameters['learning_rate'],\n",
    "                'metric_name': metric_name,\n",
    "                'rmse': performance\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lst[0]['performance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('param_grid_result_lst.pkl', 'wb') as f:\n",
    "    pickle.dump(param_grid_result_lst, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperimpute.plugins.imputers import Imputers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "with open('param_grid_result_lst.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "not_nan_results = []\n",
    "for r in results:\n",
    "    if not np.isnan(r['rmse']):\n",
    "        not_nan_results.append(r)\n",
    "print(len(not_nan_results))\n",
    "print(not_nan_results[0].keys())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_rate = 0.1\n",
    "file_name='combined_oura.csv' # or \"user_581_survey_data.csv\"\n",
    "file_name = \"user_581_survey_data.csv\"\n",
    "# for r in not_nan_results:\n",
    "#     if r['missing_rate'] == missing_rate:\n",
    "#         print(type(r['rmse']), r['rmse'], np.isnan(r['rmse']))\n",
    "filtered_results = [x for x in not_nan_results if x['missing_rate']==missing_rate and x['file_name']==file_name]\n",
    "min(filtered_results, key=lambda x: x['rmse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
