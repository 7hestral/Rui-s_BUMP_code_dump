{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "import csv\n",
    "import io\n",
    "from io import StringIO, BytesIO, TextIOWrapper\n",
    "import gzip\n",
    "from datetime import datetime, date\n",
    "from src.s3_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import ast\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score\n",
    "from xgboost.sklearn import XGBClassifier, XGBRegressor\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "import sys\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.autograd import Variable\n",
    "from pathlib import Path\n",
    "import random\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, feature_size, n_state, hidden_size, rnn=\"GRU\", regres=True, bidirectional=False, return_all=False,\n",
    "                 seed=random.seed('2021')):\n",
    "        \n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_state = n_state\n",
    "        self.seed = seed\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.rnn_type = rnn\n",
    "        self.regres = regres\n",
    "        self.return_all = return_all\n",
    "        \n",
    "        # Input to torch LSTM should be of size (seq_len, batch, input_size)\n",
    "        if self.rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(feature_size, self.hidden_size, bidirectional=bidirectional, batch_first=True).to(self.device)\n",
    "        else:\n",
    "            self.rnn = nn.LSTM(feature_size, self.hidden_size, bidirectional=bidirectional, batch_first=True).to(self.device)\n",
    "\n",
    "        self.regressor = nn.Sequential(nn.BatchNorm1d(num_features=self.hidden_size),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Dropout(0.1),\n",
    "                                       nn.Linear(self.hidden_size, self.n_state),\n",
    "                                       nn.Sigmoid())\n",
    "\n",
    "    def forward(self, input, past_state=None, **kwargs):\n",
    "        input = input.to(self.device)\n",
    "        self.rnn.to(self.device)\n",
    "        self.regressor.to(self.device)\n",
    "        if not past_state:\n",
    "            #  hidden states: (num_layers * num_directions, batch, hidden_size)\n",
    "            past_state = torch.zeros([1, input.shape[0], self.hidden_size]).to(self.device)\n",
    "        if self.rnn_type == 'GRU':\n",
    "            all_encodings, encoding = self.rnn(input, past_state)\n",
    "        else:\n",
    "            all_encodings, (encoding, state) = self.rnn(input, (past_state, past_state))\n",
    "        \n",
    "        if self.regres:\n",
    "            if not self.return_all:\n",
    "                return self.regressor(encoding.view(encoding.shape[1], -1))\n",
    "            else:\n",
    "                reshaped_encodings = all_encodings.view(all_encodings.shape[1]*all_encodings.shape[0],-1)\n",
    "                return torch.t(self.regressor(reshaped_encodings).view(all_encodings.shape[0],-1))\n",
    "        else:\n",
    "            return encoding.view(encoding.shape[1], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_model(model, train_dataloader, n_epochs, lr):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.0001)\n",
    "    criterion = nn.BCELoss().to(device)\n",
    "\n",
    "    history = dict(train=[], val=[])\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    iters,iters_sub, train_acc, val_acc = [], [] ,[], []\n",
    "    \n",
    "    best_loss = 10000.0\n",
    "    \n",
    "    n=0\n",
    "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "        t0 = time.time()\n",
    "        # print(\"\")\n",
    "        # print('======== Epoch {:} / {:} ========'.format(epoch, n_epochs))\n",
    "        # print('Training...')\n",
    "\n",
    "        total_train_loss = 0\n",
    "        train_losses=[]\n",
    "        model = model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                elapsed = (time.time() - t0)\n",
    "                # print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "            b_input= batch[0].to(device)\n",
    "            target =  batch[1].to(device)\n",
    "            iters.append(n)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(b_input)\n",
    "            out = torch.transpose(out,1,0)[0]\n",
    "\n",
    "            # target = torch.argmax(b_target, 1)\n",
    "            loss = criterion(out, target)\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            \n",
    "            if n % 10 == 0:\n",
    "                iters_sub.append(n)\n",
    "                \n",
    "                #train_acc.append(get_accuracy(model, train_dataloader))\n",
    "                #print(get_accuracy(model, train_dataloader))\n",
    "                #val_acc.append(get_accuracy(model, validation_dataloader))\n",
    "            # increment the iteration number\n",
    "            n += 1\n",
    "\n",
    "        training_time = (time.time() - t0)\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        # print(\"\")\n",
    "        # print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        # print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "\n",
    "        # print(\"\")\n",
    "        # print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "        total_eval_accuracy = 0\n",
    "        total_eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "        val_losses = []\n",
    "        model = model.eval()\n",
    "\n",
    "        train_loss = np.mean(train_losses)\n",
    "    \n",
    "        history['train'].append(train_loss)\n",
    "        \n",
    "        # print(f'Epoch {epoch}: train loss {train_loss} ')\n",
    "    plt.style.use('seaborn-white')\n",
    "    plt.plot(history['train'])\n",
    "\n",
    "    plt.title('LSTM  Training Curves')\n",
    "    plt.ylabel('CE Loss')\n",
    "    plt.xlabel('Epoch Number')\n",
    "    plt.legend(['Train', 'Val'], loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    return model.eval(), history\n",
    "\n",
    "def evaluate_model(model, x_test, y_test):\n",
    "    #INFERENCE ON TEST SET\n",
    "    x_test.to(device)\n",
    "    y_test.to(device)\n",
    "    predictions = model(x_test).detach().numpy()\n",
    "    \n",
    "    y_test = y_test.numpy()\n",
    "    y_pred = predictions.round()\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    print(f'accuracy: {accuracy}')\n",
    "    print(f'f1: {f1}')\n",
    "    print(f'precision: {precision}')\n",
    "    return y_pred, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/daily/train_10days.pickle', 'rb') as handle:\n",
    "    train = pickle.load(handle)\n",
    "with open('data/daily/test_10days.pickle', 'rb') as handle:\n",
    "    test = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train = train['X']\n",
    "X_train_scaled = np.zeros_like(X_train)\n",
    "y_train = np.array(train['y'])\n",
    "X_test = test['X']\n",
    "X_test_scaled = np.zeros_like(X_test)\n",
    "y_test = np.array(test['y'])\n",
    "\n",
    "for i in range(X_train.shape[1]):\n",
    "    scaler = StandardScaler().fit(X_train[:,i,:])\n",
    "    X_train_scaled[:,i,:] = scaler.transform(X_train[:,i,:])\n",
    "    X_test_scaled[:,i,:] = scaler.transform(X_test[:,i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape #(sample size, #feature, #time steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(test['uid']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train) - np.count_nonzero(y_train) + len(y_test) - np.count_nonzero(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "#Flipping so it goes batchsize, seq len, num features\n",
    "X_train_scaled = torch.flip(torch.tensor(X_train, dtype=torch.float), [1,2])\n",
    "y_train = torch.tensor(y_train, dtype=torch.float)\n",
    "\n",
    "X_test_scaled = torch.flip(torch.tensor(X_test, dtype=torch.float), [1,2])\n",
    "y_test = torch.tensor(y_test, dtype=torch.float)\n",
    "\n",
    "train_dataset= TensorDataset(X_train_scaled, y_train)\n",
    "\n",
    "test_dataset= TensorDataset(X_test_scaled, y_test)\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  \n",
    "            sampler = RandomSampler(train_dataset), \n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "            test_dataset, \n",
    "            sampler = SequentialSampler(test_dataset)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(X_train.shape[2], #num features \n",
    "                1, #num classes,\n",
    "                20, #hidden size\n",
    "                rnn=\"lstm\" #rnn type\n",
    "           )\n",
    "model.to(device)\n",
    "model, history = train_model(model, train_dataloader, 6000, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, output = evaluate_model(model, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = []\n",
    "for i in output:\n",
    "    if i[0] < 0.5:\n",
    "        confidence.append(1 - i[0])\n",
    "    else:\n",
    "        confidence.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, output.T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'user_id': test['uid'],\n",
    "    'ground_truth': y_test.tolist(),\n",
    "    'prediction': y_pred.T[0].tolist(),\n",
    "    'correctness': (y_test.numpy() == y_pred.T[0]).tolist(),\n",
    "    'y_score': output.T[0].tolist(),\n",
    "    'confidence': confidence,\n",
    "    'x_number_of_days_before_delivery': test['start_date']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_excel(\"results/daily/medium_window_30days.xlsx\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
